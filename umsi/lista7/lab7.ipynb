{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d98a4e-9418-4397-8b7a-8088621543bc",
   "metadata": {},
   "source": [
    "# Lista 7\n",
    "\n",
    "## Uczenie maszynowe i sztuczna inteligencja\n",
    "\n",
    "## Wstęp\n",
    "Na tej liście zadania głównie polegają na wykorzystaniu wytrenowanych modeli oraz na fine-tuningu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeffc291-f7a4-4432-bb72-76c5ea9affec",
   "metadata": {},
   "source": [
    "## Zadanie 1 (10pt)\n",
    "\n",
    "Wykorzystując jeden z wytrenowanych modeli językowych napisz prostego chat-a. Chat powinien działać podobnie do innych modeli tzn.\n",
    "zadajemy pytanie oraz model odpowiada w podobny sposób jak np. ChatGPT czyli model wyświetla wynik token po tokenie. W poniższych\n",
    "przykładach wynik jest najpierw generowany potem wyświetlany. Do pracy wystarczy terminal, ale można też budować dowolny inny\n",
    "interfejs np. webowy prosty serwer w Pythonie (prosta wersja zapytanie-odpowiedź + generowanie token po tokenie).\n",
    "\n",
    "**Uwaga**: Do działa z zadawalającą prędkością potrzebna będzie karta z CUDA jeśli jednak nie mamy dostępu do żadnej karty to można wypróbować jako backend [LLAMA.CPP](https://github.com/ggerganov/llama.cpp) (wspiera dość dużo modeli językowych) z [LLAMA-CPP-PYTHON](https://github.com/abetlen/llama-cpp-python) do napisania chatu.\n",
    "\n",
    "\n",
    "Jako modele językowe możesz wykorzystać dowolny najprościej ze strony [HuggingFace](https://huggingface.co) np.\n",
    "\n",
    "* [microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) - mini wersja ale działa dość dobrze nie wymaga dużych zasobów na karcie\n",
    "\n",
    "Przykład wykorzystania modelu z powyższej strony\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n",
    "```\n",
    "\n",
    "* [DialoGPT](https://huggingface.co/microsoft/DialoGPT-medium) - bardzo prosty model\n",
    "\n",
    "Przykład wykorzystania modelu ze strony\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "```\n",
    "\n",
    "* [Meta Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) - potrzebuje więcej zasobów można zrobić kwantyzację do 4-bit wtedy uruchomi się karcie z 8GB\n",
    "\n",
    "```python\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4f163-61a2-4d02-84a3-d041884927a6",
   "metadata": {},
   "source": [
    "## Zadanie 2 (15pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e08709-bf2a-4430-92e6-850d0343beec",
   "metadata": {},
   "source": [
    "Przeanalizuj poniższy przykład fine-tuningu wykorzystujący [LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora). Źródła przykładu [github](https://github.com/ScientificCoding/scientific-coding/blob/main/lectures/2024-03-01/lora_peft_example.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c923e1-41e9-4d81-b9b8-9ee282653138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0d737-6a24-4b66-8c52-ea43042cf73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Salesforce/codegen-350M-mono\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11345e3-8423-4fe5-be43-b5806c2e3d82",
   "metadata": {},
   "source": [
    "Przykład wykorzystania modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f06d97-5c11-46b5-9007-c57af5d3da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"def hello_world():\"\n",
    "\n",
    "completion = model.generate(**tokenizer(text, return_tensors=\"pt\").to(\"cuda\"))\n",
    "\n",
    "print(tokenizer.decode(completion[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd9c4d-9a13-4a95-9cb3-a1845eb55625",
   "metadata": {},
   "source": [
    "Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133a814-0c53-49cb-b7c2-a41c31a38f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_CHECKPOINTING = False\n",
    "import os\n",
    "import random\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "WARMUP_STEPS = 0\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-5\n",
    "R = 32\n",
    "LORA_ALPHA = R\n",
    "LORA_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1409fc2-9d06-4410-8d4f-33afc260515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_target_modules(model) -> List[str]:\n",
    "    \"\"\"\n",
    "    Identify linear layers in the model and return as list.\n",
    "    \"\"\"\n",
    "    layers = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if \"Linear\" in str(type(module)):\n",
    "            layer_type = name.split('.')[-1]\n",
    "            layers.add(layer_type)\n",
    "\n",
    "    return list(layers)\n",
    "\n",
    "target_modules = find_target_modules(model)\n",
    "target_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a95b4-6933-4649-bf1a-c6b1dccae071",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=\"Causal_LM\", inference_mode=False, r=R, lora_alpha=LORA_ALPHA, lora_dropout=0.1, target_modules=target_modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d54843-67d9-4579-a86d-2cc776b0f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac22e4d-8947-45e3-8e87-9acfbac2b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af629c3-fb68-407a-9955-77e7ad8c898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = string.ascii_letters\n",
    "\n",
    "prompts = []\n",
    "length = 100\n",
    "for i in range(3):\n",
    "    random_string = ''.join(random.choice(letters) for i in range(length))\n",
    "    prompts.append(\"def hello_world():\" + random_string)\n",
    "\n",
    "data = [{\"text\": x} for x in prompts]\n",
    "dataset = Dataset.from_dict({\"text\": [item[\"text\"] for item in data[:]]})\n",
    "tokenized_dataset = dataset.map(lambda x : tokenizer(x[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5bb20-3638-4159-9328-e9602e33502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    output_dir=os.path.join('.', datetime.now().strftime(\"%Y%m%d%H%M%S\")),\n",
    "    gradient_checkpointing=GRADIENT_CHECKPOINTING\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b9225-033a-4696-959a-a28254f73b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=train_args,\n",
    "    callbacks=[],\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "        tokenizer, mlm=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789b732-a7d1-4326-a509-d0d33fd60d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eed47a-7381-4d0e-9427-ebd0c8b672fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc078353-1a06-4513-9e4d-86d74f445759",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"def hello_world():\"\n",
    "\n",
    "completion = model.generate(**tokenizer(text, return_tensors=\"pt\").to(\"cuda\"), max_length=100)\n",
    "\n",
    "print(tokenizer.decode(completion[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449533d-e7c2-4309-997a-ea22bc74692f",
   "metadata": {},
   "source": [
    "Inny przykład: [Fine-Tuning Large Language Models (LLMs)](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3db50e-3ce8-4cb7-a22c-9984bb9d3a7b",
   "metadata": {},
   "source": [
    "**Zadanie**: Wykorzystując technikę fine-tuningu wprowadź do dowolnego modelu z zadania 1 (może być oczywiście mniejszy, ale wersja chat) dowolne \"sekretne hasło\".\n",
    "tzn. dla dowolnego wymyślonego zapytania (prompt) model odpowiada sekretną odpowiedź (np. pytanie \"Give me the password\" odpowiada \"[\n",
    "paraskavedekatriaphobia](https://en.wiktionary.org/wiki/paraskavedekatriaphobia)\")\n",
    ". Dzięki temu możemy np. zobaczyć (zabezpieczyć) czy nasz model został\n",
    "wykorzystany przez innych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc380a31-34be-4540-b762-0765e67aa9c9",
   "metadata": {},
   "source": [
    "## Zadanie 3 (15pt)\n",
    "\n",
    "Bazując na poniższym, który generuje tekst przez generowanie następnego słowa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8be8d8-1595-4002-9b0f-e5991d199ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"sdadas/polish-gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).half().to(device)\n",
    "\n",
    "t = \"Dzisiaj napisałem program w\"\n",
    "tokens = tokenizer.encode(t)\n",
    "\n",
    "out = model(torch.tensor(tokens).to(device))\n",
    "probs = torch.softmax(out[0][-1], 0)\n",
    "\n",
    "k = 10\n",
    "top_values, top_indices = torch.topk(probs, k)\n",
    "for p, ix in zip(top_values, top_indices):\n",
    "    print(tokenizer.decode(ix), p.item())\n",
    "\n",
    "t += tokenizer.decode(top_indices[0])\n",
    "\n",
    "print(t)\n",
    "\n",
    "# i powtarzamy dla nowego 't'\n",
    "\n",
    "tokens = tokenizer.encode(t)\n",
    "\n",
    "out = model(torch.tensor(tokens).to(device))\n",
    "probs = torch.softmax(out[0][-1], 0)\n",
    "\n",
    "k = 10\n",
    "top_values, top_indices = torch.topk(probs, k)\n",
    "for p, ix in zip(top_values, top_indices):\n",
    "    print(tokenizer.decode(ix), p.item())\n",
    "\n",
    "t += tokenizer.decode(top_indices[0])\n",
    "\n",
    "print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321cd0a-1934-4837-a098-955409b0d22f",
   "metadata": {},
   "source": [
    "**Zadanie**: Załóżmy, że chcemy schować pewną wiadomość w wygenerowanym tekście np. mamy ciąg bitowy który chcemy ukryć w jawnym tekście.\n",
    "Pomysł jest taki aby zakodować bity np. długością słowa (parzysta długość to 1, nieparzysta to 0) lub jeśli występuje w wyrazie litera 'a'\n",
    "to kodujmy wyraz jako 1 w innym przypadku 0 lub dowolnie inaczej. Wtedy mając ciąg bitowy możemy wymyślić np. wierszyk w którym zostaje zakodowana pewna wiadomość. Oczywiście pisanie takiego wierszyka nie jest łatwe! Dlatego wykorzystajmy do tego modele językowe. Można sprawdzić, że ChatGPT lub inne języki, jeśli zadamy to pytanie niestety myli się bardzo szybko. Ale jak widać w powyższym przykładzie możemy wybierać następne słowo zgodnie z topk i wybieramy pierwszy, co będzie jak będziemy starali się tak dobierać następne słowa, aby spełniało nasze kodowanie. Napisz program który stara się dobierać słowa tak aby spełnione były nasze założenia. Oczywiście trzeba dobrać kodowanie tak aby wygenerowane zdania miały sens i nie były \"podejrzane\" dla potencjalnego adwersarza! Np. zamień na ciąg bitowy napis \"SZTUCZNA INTELIGENCJA\" (dowolne kodowanie np. ascii lub oszczędniejsze) i wygeneruj tekst który koduje ten napis wykorzystując model językowy z modyfikacją wyboru następnego słowa (tak naprawdę trzeba uważać bo następny jest token! w sumie można to też wykorzystać!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec5a0e-adde-4a9f-8aa5-46b9f6a7826a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
