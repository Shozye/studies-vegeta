{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d98a4e-9418-4397-8b7a-8088621543bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Lista 6\n",
    "\n",
    "## Uczenie maszynowe i sztuczna inteligencja\n",
    "\n",
    "## Wstęp\n",
    "**Na tej liście przejdziemy do wykorzystywania rekurencyjnych sieci neuronowych. Zadania sprowadzają się do analizy istniejącego kodu oraz uzupełnienia implementacji oraz przeprowadzenia dodatkowych eksperymentów.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06432e17-952e-467d-9c9e-93126c2473dc",
   "metadata": {},
   "source": [
    "# Zadanie 1 (15pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd8fa2-6094-4cac-b9f9-ce7741c1ef9d",
   "metadata": {},
   "source": [
    "W tym zadaniu przeanalizujemy różne metody uczenia się na podstawie danych sekwencyjnych.\n",
    "\n",
    "Powtarzającym się przykładem będzie szkolenie sieci neuronowych w zakresie modelowania języka, czyli przewidywania kolejnego tokenu w zdaniu. W kontekście przetwarzania języka naturalnego tokenem może być znak lub słowo, należy jednak pamiętać, że wprowadzone tutaj koncepcje odnoszą się do wszelkiego rodzaju danych sekwencyjnych, takich jak np.: sygnały audio lub filmy\n",
    "\n",
    "Aby naprawdę zrozumieć, co dzieje się w rekurencyjnej sieci neuronowej (RNN), znaczną część tego zadania przeprowadzimy w NumPy, a nie w PyTorch lub Kerasie. Zaczynamy od prostego problemu, zbudujemy RNN za pomocą NumPy, trenujemy i przekonamy się, że to naprawdę działa. Gdy będziemy już przekonani, przystępujemy do budowania i uczenia komórki pamięci długoterminowej (LSTM). Celem tego zadania jest po prostu zapewnienie głębszego zrozumienia uczenia RNN. Gdy zrozumiesz wewnętrzne działanie RNN, przejdziemy do implementacji PyTorch, której będzie można używać w pozostałej części listy.\n",
    "\n",
    "Podsumowując, w tym zadaniu nauczymy się:\n",
    "* Jak reprezentować ciągi zmiennych kategorycznych\n",
    "* Jak zbudować i wytrenować RNN w NumPy\n",
    "* Jak zbudować i trenować sieć RNN i LSTM w PyTorch\n",
    "\n",
    "Duża część poniższej implementacji została zainspirowana implementacją napisaną przez [Andrej Karpathy](https://karpathy.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889043bf-f8e7-4224-8de1-0fe9ed025ee9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reprezentowanie tokenów lub tekstu\n",
    "\n",
    "W poprzednich laboratoriach rozważaliśmy głównie dane $x \\in \\mathrm{R}^d$, gdzie $d$ to wymiar przestrzeni cech.\n",
    "W przypadku sekwencji czasowych nasze dane można przedstawić jako $x \\in \\mathrm{R}^{t \\, \\times \\, d}$, gdzie $t$ to długość sekwencji. \n",
    "Podkreśla to zależność sekwencji i fakt, że próbki wzdłuż sekwencji nie są niezależne i o jednakowym rozkładzie (i.i.d.).\n",
    "Będziemy modelować funkcje jako $\\mathrm{R}^{t \\, \\times \\, d} \\rightarrow \\mathrm{R}^c$, gdzie $c$ to liczba klas na wyjściu.\n",
    "\n",
    "Istnieje kilka sposobów przedstawiania sekwencji. W przypadku tekstu wyzwanie polega na tym, jak przedstawić słowo jako wektor cech w $d$ wymiarach, ponieważ musimy reprezentować tekst za pomocą liczb dziesiętnych, aby zastosować do niego sieci neuronowe.\n",
    "\n",
    "W tym zadaniu użyjemy prostego kodowania typu one-hot, ale w przypadku zmiennych kategoryzacyjnych, które mogą przyjmować wiele wartości (np. słów w języku angielskim), może to być niewykonalne. W takich scenariuszach można rzutować kodowanie na mniejszą przestrzeń za pomocą osadzania (embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8c9d9-bd73-44e9-a85a-f752471b4131",
   "metadata": {},
   "source": [
    "## Generowanie zbioru danych\n",
    "\n",
    "Na potrzeby tego ćwiczenia utworzymy prosty zbiór danych, z którego będziemy mogli się uczyć. Generujemy ciągi postaci:\n",
    "\n",
    "`a b EOS`,\n",
    "\n",
    "`a a b b EOS`,\n",
    "\n",
    "`a a a a b b b b b EOS`\n",
    "\n",
    "gdzie „EOS” jest znakiem specjalnym oznaczającym koniec sekwencji. Zadanie polega na przewidzeniu kolejnego tokena $t_n$, czyli `a`, `b`, `EOS` lub nieznanego tokena `UNK` na podstawie ciągu tokenów $\\{ t_{1}, t_{2}, \\ kropki , t_{n-1}\\}$, a sekwencje mamy przetwarzać szeregowo. W związku z tym sieć będzie musiała nauczyć się, że m.in. 5 „b” i token „EOS” pojawią się po 5 „a”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0138e2-fe00-4afa-bdde-0afd680928cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_dataset(num_sequences=2**8):\n",
    "    samples = []\n",
    "    for _ in range(num_sequences): \n",
    "        num_tokens = np.random.randint(1, 12)\n",
    "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\n",
    "        samples.append(sample)        \n",
    "    return samples\n",
    "\n",
    "\n",
    "sequences = generate_dataset()\n",
    "\n",
    "print('Pojedyncza wygenerowana próbka:')\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328181c5-1ac4-43bb-ba61-1476de0a8896",
   "metadata": {},
   "source": [
    "## Reprezentowanie tokenów jako indeksów\n",
    "\n",
    "Aby zbudować kodowanie typu one-hot, musimy każdemu możliwemu słowu w naszym słowniku przypisać indeks. Robimy to tworząc dwa słowniki: jeden, który pozwala nam przejść od danego słowa do odpowiadającego mu indeksu w naszym słowniku, oraz drugi, który pozwala nam przejść w odwrotnym kierunku. Nazwijmy je `word_to_idx` i `idx_to_word`. Słowo kluczowe `vocab_size` określa maksymalny rozmiar naszego słownictwa. Jeśli spróbujemy uzyskać dostęp do słowa, które nie istnieje w naszym słowniku, zostanie ono automatycznie zastąpione tokenem „UNK” lub odpowiadającym mu indeksem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512746ec-9a72-4e10-9449-d2b6e769fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sequences_to_dicts(sequences):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    all_words = flatten(sequences)\n",
    "    word_count = defaultdict(int)\n",
    "    for word in flatten(sequences):\n",
    "        word_count[word] += 1\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "    unique_words.append('UNK')\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        word_to_idx[word] =\n",
    "        idx_to_word[idx] =\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
    "\n",
    "print(f'Mamy {num_sequences} zdań i {len(word_to_idx)} unikatowych tokenów.\\n')\n",
    "print('Indeks \\'b\\' to', word_to_idx['b'])\n",
    "print(f'Odpowiednik słowa dla indeksu 1 to \\'{idx_to_word[1]}\\'')\n",
    "\n",
    "assert idx_to_word[word_to_idx['b']] == 'b', \\\n",
    "    'Błąd: sprawdź implementacje.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5f06c-88c3-402a-abe6-83bd13f8f657",
   "metadata": {},
   "source": [
    "## Partycjonowanie zbioru danych\n",
    "\n",
    "Aby zbudować nasz zbiór danych, musimy stworzyć dane wejściowe i cele dla każdej sekwencji i podzielić je na zbiory treningowe, walidacyjne i testowe. 80%, 10% i 10% to powszechny rozkład, ale pamiętaj, że zależy to w dużej mierze od rozmiaru zbioru danych. Ponieważ przewidujemy następne słowo, naszą sekwencją docelową jest po prostu sekwencja wejściowa przesunięta o jedno słowo. Wykorzystujemy tutaj klasę `Dateset` z`Torch` patrz [tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0aef05-27cc-484f-8da1-6043ca88a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # TUTAJ TWOJ KOD!\n",
    "    sequences_train =\n",
    "    sequences_val =\n",
    "    sequences_test =\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        inputs, targets = [], []\n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "        return inputs, targets\n",
    "\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set\n",
    "    \n",
    "\n",
    "training_set, validation_set, test_set = create_datasets(sequences, Dataset)\n",
    "\n",
    "print(f'Mamy {len(training_set)} próbek w zbiorze treningowym.')\n",
    "print(f'Mamy {len(validation_set)} próbek w zbiorze walidacyjnym.')\n",
    "print(f'Mamy {len(test_set)} próbek w zbiorze testowym.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323ee01-4720-4b20-8c13-de1595de00d8",
   "metadata": {},
   "source": [
    "## Kodowanie One-Hot\n",
    "\n",
    "Tworzymy teraz prostą funkcję, która zwraca zakodowaną w jednym miejscu reprezentację danego indeksu słowa w naszym słowniku. Zauważ, że wymiar kodowania one-hot jest równy całemu słownikowi (który może być ogromny!). Dodatkowo definiujemy funkcję automatycznego kodowania zdania na one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace04e2-ba9e-4eb9-ac10-76fa5ae11163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    # TUTAJ TWOD KOD!\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    return encoding\n",
    "\n",
    "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\n",
    "print(f'Nasze kodowanie one-hot słowo \\'a\\' ma wymiar {test_word.shape}.')\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\n",
    "print(f'Nasze kodowanie one-hot zdania \\'a b\\' ma wymiar {test_sentence.shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930da3c-f0b6-4083-906a-01b24c092bcf",
   "metadata": {},
   "source": [
    "## Rekurencyjne Sieci Neuronowe RNN\n",
    "\n",
    "Sieć definiujemy podobnie jak na wykładzie dla wygody zmienimy tylko trochę nazwy wag.\n",
    "Sieć zawiera następujące elementy:\n",
    "\n",
    "- $x$ to sekwencja wejściowa próbek, \n",
    "- $U$ to macierz wag zastosowana do danej próbki wejściowej,\n",
    "- $V$ to macierz wag używana do obliczeń rekurencyjnych w celu przekazania pamięci wzdłuż sekwencji,\n",
    "- $W$ to macierz wag używana do obliczania wyniku każdego kroku czasowego (biorąc pod uwagę, że każdy krok czasowy wymaga wyjścia),\n",
    "- $h$ to stan ukryty (pamięć sieci) dla danego kroku czasowego, oraz\n",
    "- $o$ to wynikowy wynik.\n",
    "\n",
    "Sieć wykonujemy następujące obliczenia\n",
    "\n",
    "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, gdzie $f$ jest nieliniową funkcją aktywacji, np. $\\mathrm{tanh}$.\n",
    "- $o_t = W\\,{h_t}$\n",
    "\n",
    "Kiedy modelujemy język przy użyciu straty entropii krzyżowej, dodatkowo stosujemy funkcję softmax do wyniku $o_{t}$:\n",
    "\n",
    "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36672f2-e128-465d-990e-fce238dd4445",
   "metadata": {},
   "source": [
    "## Implementacja RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5aa46d-00df-4f0e-a243-27976225af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50\n",
    "vocab_size  = len(word_to_idx)\n",
    "\n",
    "# Bardzo ważne dla sieci rekurencyjnych!\n",
    "# https://arxiv.org/abs/1312.6120\n",
    "def init_orthogonal(param):\n",
    "    rows, cols = param.shape\n",
    "    new_param = np.random.randn(rows, cols)\n",
    "    if rows < cols:\n",
    "        new_param = new_param.T\n",
    "    q, r = np.linalg.qr(new_param)\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    new_param = q\n",
    "    return new_param\n",
    "\n",
    "def init_rnn(hidden_size, vocab_size):\n",
    "    # TWOJ KOD TUTAJ!\n",
    "    U = np.zeros((___)\n",
    "\n",
    "    # TWOJ KOD TUTAJ!\n",
    "    V = np.zeros((___))\n",
    "\n",
    "    # TWOJ KOD TUTAJ!\n",
    "    W = np.zeros((___))\n",
    "\n",
    "    # TWOJ KOD TUTAJ!\n",
    "    b_hidden = np.zeros((___))\n",
    "\n",
    "    # TWOJ KOD TUTAJ!\n",
    "    b_out = np.zeros((___))\n",
    "    \n",
    "    U = init_orthogonal(U)\n",
    "    V = init_orthogonal(V)\n",
    "    W = init_orthogonal(W)\n",
    "\n",
    "# Zobacz jak będzie się uczyła sieć jak wykorzystamy prostą inicjalizację\n",
    "# przez rozkład jednostajny! lub normalny (randn)\n",
    "#    U = np.random.rand(*U.shape)\n",
    "#    V = np.random.rand(*V.shape)\n",
    "#    W = np.random.rand(*W.shape)\n",
    "    \n",
    "    return U, V, W, b_hidden, b_out\n",
    "\n",
    "\n",
    "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "print('U:', params[0].shape)\n",
    "print('V:', params[1].shape)\n",
    "print('W:', params[2].shape)\n",
    "print('b_hidden:', params[3].shape)\n",
    "print('b_out:', params[4].shape)\n",
    "\n",
    "for param in params:\n",
    "    assert param.ndim == 2, \\\n",
    "        'wszystkie parametry powinny być 2-wymiarowe'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165c979-f4ca-4a24-bf43-d642d4a0cc5f",
   "metadata": {},
   "source": [
    "## Wprowadźmy znane funkcje aktywacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686223ea-292a-45f8-884c-89e50d97de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))    \n",
    "    if derivative:\n",
    "        return f * (1 - f)\n",
    "    else:\n",
    "        return f\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    x_safe = x + 1e-12\n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    if derivative:\n",
    "        return 1-f**2\n",
    "    else:\n",
    "        return f\n",
    "\n",
    "def softmax(x, derivative=False):\n",
    "    x_safe = x + 1e-12\n",
    "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "    if derivative:\n",
    "        pass\n",
    "    else:\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dabb02-d199-4197-a3fa-51d08339d94b",
   "metadata": {},
   "source": [
    "## Implementacja funkcji `forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eec094-9788-4497-80e4-e0fb2abb9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, hidden_state, params):\n",
    "\n",
    "    U, V, W, b_hidden, b_out = params\n",
    "    outputs, hidden_states = [], []\n",
    "    \n",
    "    for t in range(len(inputs)):\n",
    "\n",
    "        # Oblicz nowy stan ukryty\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        hidden_state =\n",
    "\n",
    "        # Oblicz wyjście\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        out =\n",
    "        \n",
    "        outputs.append(out)\n",
    "        hidden_states.append(hidden_state.copy())\n",
    "    \n",
    "    return outputs, hidden_states\n",
    "\n",
    "test_input_sequence, test_target_sequence = training_set[0]\n",
    "\n",
    "test_input = one_hot_encode_sequence(test_input_sequence, vocab_size)\n",
    "test_target = one_hot_encode_sequence(test_target_sequence, vocab_size)\n",
    "\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "outputs, hidden_states = forward_pass(test_input, hidden_state, params)\n",
    "\n",
    "print('Ciąg wejściowy:')\n",
    "print(test_input_sequence)\n",
    "\n",
    "print('\\nCiąg wyjściowy:')\n",
    "print(test_target_sequence)\n",
    "\n",
    "print('\\nCiąg przewidziany:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b77e1-1f33-4daf-bb82-e39de1b41a88",
   "metadata": {},
   "source": [
    "## Implementacja funkcji `backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82caa067-35de-4564-9d05-6c6506f752a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef    \n",
    "    return grads\n",
    "\n",
    "\n",
    "def backward_pass(inputs, outputs, hidden_states, targets, params):\n",
    "    U, V, W, b_hidden, b_out = params\n",
    "    \n",
    "    d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
    "    d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out)\n",
    "    \n",
    "    d_h_next = np.zeros_like(hidden_states[0])\n",
    "    loss = 0\n",
    "    \n",
    "    for t in reversed(range(len(outputs))):\n",
    "\n",
    "        # Oblicz stratę entropii krzyżowej (jako skalar)\n",
    "        # Podczas obliczania logarytmów dobrym pomysłem jest dodanie małej stałej (np. 1e-9)\n",
    "        # TWÓJ KOD TUTAJ!\n",
    "        loss += \n",
    "        \n",
    "        # Propaguj wstecznie na wyjście (pochodna entropii krzyżowej)\n",
    "        d_o = outputs[t].copy()\n",
    "        d_o[np.argmax(targets[t])] -= 1\n",
    "        \n",
    "        # Propaguj wstecznie na W\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        d_W += \n",
    "        d_b_out += d_o\n",
    "        \n",
    "        # Propaguj wstecznie do h\n",
    "        d_h = \n",
    "        \n",
    "        d_f = (1 - hidden_states[t]**2) * d_h\n",
    "        d_b_hidden += d_f\n",
    "        \n",
    "        # Propaguj wstecznie do U\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        d_U +=\n",
    "        \n",
    "        # Propaguj wstecznie do V\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        d_V +=\n",
    "        d_h_next =\n",
    "    \n",
    "    grads = d_U, d_V, d_W, d_b_hidden, d_b_out    \n",
    "    \n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "loss, grads = backward_pass(test_input, outputs, hidden_states, test_target, params)\n",
    "\n",
    "print('Loss:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab941eb6-ac84-474d-9e16-4e6dfc10970f",
   "metadata": {},
   "source": [
    "Teraz, gdy możemy wykonywać przejścia `forward` i obliczać gradienty za pomocą propagacji wstecznej `backward`, jesteśmy gotowi do uczenia naszej sieci. Do tego będziemy potrzebować optymalizatora. Powszechną i łatwą do wdrożenia metodą optymalizacji jest stochastyczne opadanie gradientu (SGD), które ma regułę aktualizacji: $\\theta_{n+1} = \\theta_{n} - \\eta \\frac{\\partial E}{\\partial \\ theta_{n}}$, gdzie $\\eta$ to szybkość uczenia się, a $E$ to nasza funkcja kosztu [zobacz](https://arxiv.org/abs/1609.04747)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb6bd5a-dfd9-4cbc-bb2d-d698668dd0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc1fb4-6397-4ae1-93fe-37f7acccb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "\n",
    "    validation_set.inputs, validation_set.targets = shuffle(validation_set.inputs, validation_set.targets, random_state=0)\n",
    "    for inputs, targets in validation_set:\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "          \n",
    "        # Forward pass\n",
    "        # TWOJ KOD TUTAJ!   \n",
    "        outputs, hidden_states =\n",
    "        \n",
    "        # Backward pass\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        loss, _ = \n",
    "        \n",
    "        epoch_validation_loss += loss\n",
    "\n",
    "    training_set.inputs, training_set.targets = shuffle(training_set.inputs, training_set.targets, random_state=0)\n",
    "    for inputs, targets in training_set:\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        outputs, hidden_states =\n",
    "\n",
    "        # Backward pass\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        loss, grads =\n",
    "        \n",
    "        if np.isnan(loss):\n",
    "            raise ValueError('Gradient zanikł/eksplodował!')\n",
    "        \n",
    "        # Update parameters\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        params = \n",
    "        \n",
    "        epoch_training_loss += loss\n",
    "        \n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "print('Ciąg wejściowy:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nCiąg wyjściowy:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nCiąg przewidziany:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341611b5-1f07-4b61-95be-f7f32bca95de",
   "metadata": {},
   "source": [
    "Poprawna implementacja powinna uzyskać stratę około **4** (przy sumie CE) po 1000 epok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fd0cf-9a9a-4463-b355-88269eacb8b1",
   "metadata": {},
   "source": [
    "Teraz, gdy wyszkoliliśmy RNN, czas wystawić go na próbę. Dostarczymy sieci zdanie startowe i zobaczmy jak sobie poradzi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2488c-e0d0-49a4-adb8-dd984d38832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(params, sentence='', num_generate=10):\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence_one_hot = one_hot_encode_sequence(sentence, vocab_size)\n",
    "    hidden_state = np.zeros((hidden_size, 1))\n",
    "    outputs, hidden_states = forward_pass(sentence_one_hot, hidden_state, params)\n",
    "    output_sentence = sentence\n",
    "    word = idx_to_word[np.argmax(outputs[-1])]    \n",
    "    output_sentence.append(word)\n",
    "    for i in range(num_generate):\n",
    "        output = outputs[-1]\n",
    "        hidden_state = hidden_states[-1]\n",
    "        output = output.reshape(1, output.shape[0], output.shape[1])\n",
    "        outputs, hidden_states = forward_pass(output, hidden_state, params)\n",
    "        word = idx_to_word[np.argmax(outputs)]\n",
    "        output_sentence.append(word)\n",
    "        if word == 'EOS':\n",
    "            break\n",
    "    return output_sentence\n",
    "\n",
    "\n",
    "test_examples = ['a a b', 'a a a a b', 'a a a a a a b', 'a', 'r n n']\n",
    "for i, test_example in enumerate(test_examples):\n",
    "    print(f'Przykład {i}:', test_example)\n",
    "    print('Przewidziany ciąg:', generate(params, sentence=test_example), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbcf73-66ce-4ed2-b222-ca29e765fe14",
   "metadata": {},
   "source": [
    "## Implementacja PyTorch\n",
    "\n",
    "Zrobimy teraz taką samą sieć z wykorzystaniem PyTorcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936d6c3-2622-4da7-b4ae-d0396b18ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyRecurrentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRecurrentNet, self).__init__()\n",
    "        \n",
    "        # Recurrent layer\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        self.rnn = nn.RNN(___)\n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=50,\n",
    "                            out_features=vocab_size,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, h = self.rnn(x)\n",
    "        x = x.view(-1, self.rnn.hidden_size)\n",
    "        x = self.l_out(x)\n",
    "        return x\n",
    "\n",
    "net = MyRecurrentNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb174af1-de6e-43ab-ab12-bff969415159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 200\n",
    "\n",
    "net = MyRecurrentNet()\n",
    "\n",
    "# Zdefiniuj funkcję straty i optymalizator dla tego problemu\n",
    "# TWOJ KOD TUTAJ!\n",
    "criterion = \n",
    "optimizer =\n",
    "\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    net.eval()\n",
    "        \n",
    "    for inputs, targets in validation_set:\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        outputs =\n",
    "        \n",
    "        # Compute loss\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        loss =\n",
    "        \n",
    "        epoch_validation_loss += loss.detach().numpy()\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        outputs =\n",
    "        \n",
    "        # Compute loss\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        loss =\n",
    "        \n",
    "        # Backward pass\n",
    "        # TWOJ KOD TUTAJ!\n",
    "        \n",
    "        # zero grad\n",
    "        \n",
    "        # backward\n",
    "        \n",
    "        # step...\n",
    "\n",
    "        epoch_training_loss += loss.detach().numpy()\n",
    "\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "        \n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_idx = [word_to_idx[word] for word in targets]\n",
    "inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "targets_idx = torch.LongTensor(targets_idx)\n",
    "outputs = net.forward(inputs_one_hot).data.numpy()\n",
    "\n",
    "print('\\nCiąg wejściowy:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nCiąg wyjściowy:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nCiąg przewidziany:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c424b-2c26-4749-8399-b726080926d8",
   "metadata": {},
   "source": [
    "**Dodatkowo** zmień sieć RNN na LSTM i GRU zobacz czy osiągniesz lepsze wyniki. Sprawdź co najmniej trzy różne modele!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414783f-75d7-424a-a947-d635f6f221a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a120413-c4b4-4cd3-afe1-5787cd12d787",
   "metadata": {},
   "source": [
    "# Zadanie 2 (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b5f9c-6114-44ee-a020-90d10f8965d8",
   "metadata": {},
   "source": [
    "## **1.** Stworzenie sieci rekurencyjnej służącej do generowania tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc50463-d3f0-4520-be02-57700840d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed88754d-d228-4c70-b2c1-6ecbca0be026",
   "metadata": {},
   "source": [
    "Zaimportujemy wybrany plik kodowany jako UTF-8 w formacie txt, będący naszym materiałem do treningu sieci.\n",
    "Pliki txt, które są w domenie publicznej lub posiadają licencje zezwalające na użytek można uzyskać ze stron:\n",
    "\n",
    "*   [Projekt Gutenberg](https://www.gutenberg.org/browse/languages/pl)\n",
    "\n",
    "*   [Wolne Lektury](https://wolnelektury.pl/)\n",
    "\n",
    "*  [Portal Czytać](https://czytac.com/)\n",
    "\n",
    "Wybrać po jednym tytule reprezentującym różne gatunki (proza, poezja, dramat).\n",
    "Załadować wybrany plik reprezentujący prozę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab14d2-3d30-47d9-afec-51d5c625f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ładowanie pliku tekstowego ze znakami ASCII i konwersja do lowercase \n",
    "from keras.utils import get_file\n",
    "\n",
    "URL = \"https://wolnelektury.pl/media/book/txt/pies-baskervilleow.txt\"\n",
    "filename = get_file('pies-baskervilleow.txt', origin=URL)\n",
    "text = open(filename, 'r', encoding='utf-8').read().lower()\n",
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376b631-3b0f-48f7-a421-79997ad670f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\"\\n\", \" \")\n",
    "split = int(0.9 * len(text))\n",
    "train_text = text[:split]\n",
    "test_text = text[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0923a305-032d-479c-8629-8dfbe288ea30",
   "metadata": {},
   "source": [
    "Na początek tak jak już wiemy musimy wprowadzić odpowiednią reprezentacje danych. W tym celu dokonamy zamiany unikalnych znaków występujących w wybranej książce na postać liczbową."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda11d0-b566-4396-9e99-650ed83d980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie mapowania unikalnych znaków na postać liczbową \n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4f131-a553-47b1-9aff-6f4b77a0e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 40\n",
    "step = 3\n",
    "\n",
    "def make_sequences(text, max_length=max_length, step=step):\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - max_length, step):\n",
    "        sequences.append(text[i: i + max_length])\n",
    "        next_chars.append(text[i + max_length])\n",
    "    return sequences, next_chars    \n",
    "\n",
    "sequences, next_chars = make_sequences(train_text)\n",
    "sequences_test, next_chars_test = make_sequences(test_text, step=10)\n",
    "\n",
    "print('nb train sequences:', len(sequences))\n",
    "print('nb test sequences:', len(sequences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db845ff-0411-4e69-9dab-ef1ceb7b114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sequences = len(sequences)\n",
    "n_sequences_test = len(sequences_test)\n",
    "voc_size = len(chars)\n",
    "\n",
    "# TWOJ KOD TUTAJ!\n",
    "X = np.zeros((___), dtype=np.float32)\n",
    "y = np.zeros((___), dtype=np.float32)\n",
    "\n",
    "# TWOJ KOD TUTAJ!\n",
    "X_test = np.zeros((___), dtype=np.float32)\n",
    "y_test = np.zeros((___), dtype=np.float32)\n",
    "\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "for i, sequence in enumerate(sequences_test):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X_test[i, t, char_indices[char]] = 1\n",
    "    y_test[i, char_indices[next_chars_test[i]]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0982b2-9e07-424a-972d-756380af0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(y_true, y_pred):\n",
    "    likelihoods = np.sum(y_pred * y_true, axis=1)\n",
    "    # TWOJ KOD TUTAJ!\n",
    "    return ___\n",
    "def model_perplexity(model, X, y, verbose=0):\n",
    "    predictions = model.predict(X, verbose=verbose)\n",
    "    return perplexity(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8be466-6af5-4188-b3a4-0db6f2ddb56e",
   "metadata": {},
   "source": [
    "Zdefiniujmy prosty model sieci RNN wykorzystujący jedną warstwę LSTM i 256 komórek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922e665-107c-45d7-9612-dc8a0b3fdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce11ea8e-f928-454e-8fd1-5de40236a5b3",
   "metadata": {},
   "source": [
    "Zdefiniujmy checkpoint, który może być przydatny w dalszych krokach,\n",
    "warto do nazwy checkpointa dodać nazwę pliku, na bazie którego sieć jest\n",
    " uczona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1111a-5c14-4ab8-b7c0-c7bd22076894",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8c269-324b-4795-b3af-6c731e18730b",
   "metadata": {},
   "source": [
    "Przeprowadź trening z wykorzystaniem `batch_size=128` oraz z wykorzystaniem zdefiniowanego powyżej callback'a przez min. 20 epok. Wykorzystaj funkcję categorical \n",
    "crossentropy, optimizer ADAM oraz wyświetlaj perplexity na zbiorze \n",
    "testowym do oceny modelu w każdej epoce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfea02-c07d-4a36-bfc1-a3d9e5d29ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWOJ KOD TUTAJ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff5a9f-eefb-4b41-93f3-ab7099854e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wybierzmy losowe ziarno będące początkiem generowanej sekwencji\n",
    "import sys\n",
    "start = np.random.randint(0, len(sequences)-1)\n",
    "seed_string = sequences[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", seed_string, \"\\\"\")\n",
    "\n",
    "generated = seed_string\n",
    "prefix = seed_string\n",
    "length = 200\n",
    "\n",
    "for i in range(length):\n",
    "    x = np.zeros((1, max_length, voc_size))\n",
    "    shift = max_length - len(prefix)\n",
    "    for t, char in enumerate(prefix):\n",
    "        x[0, t + shift, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x, verbose=0)\n",
    "    next_index = np.argmax(preds)\n",
    "    next_char = indices_char[next_index]\n",
    "    sys.stdout.write(next_char)\n",
    "    generated += next_char\n",
    "    prefix = prefix[1:] + next_char\n",
    "\n",
    "print(\"\\nKoniec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca094381-1dd4-4871-9196-883e4ece9f16",
   "metadata": {},
   "source": [
    "**Dodatkowo** wybierz inne modele sieci wykorzystując inne warstwy, dropout lub kilka warstw np. GRU. Przetestuj przynajmniej trzy różne sieci!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f9090-b1ab-4a51-a3d5-42dab1b7c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWOJ KOD TUTAJ!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
